


```{r, message = FALSE, warning=FALSE}

library(tidyverse)
library(tidymodels)
library(ranger)
library(xgboost)
library(vip)

```



```{r}

dat <- readRDS("../data/sim_full.rds") %>% filter(trial_type == "midi_to_midi") %>% select(-trial_type)

```



```{r}

# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible 
set.seed(123)

# Put 3/4 of the data into the training set 
data_split <- initial_split(dat, prop = 3/4)

# Create dataframes for the two sets:
train_data <- training(data_split) 
test_data <- testing(data_split)

```


```{r}


rec <- train_data %>% 
  recipe(sim_ratings ~ ., data = .) %>%
  step_zv(all_numeric(), -all_outcomes()) %>%
  step_corr(all_predictors(), threshold = 0.8, method = "pearson")  %>%
  prep()
print(rec)
```



```{r}

cv_folds <- 
  recipes::bake(
    rec, 
    new_data = training(data_split)
  ) %>%  
  rsample::vfold_cv(v = 5)


```



```{r}

# XGBoost model specification
xgboost_model <- 
  parsnip::boost_tree(
    mode = "regression",
    trees = 1000#,
    #min_n = tune(),
    #tree_depth = tune(),
    #learn_rate = tune(),
    #loss_reduction = tune()
  ) %>%
    set_engine("xgboost", objective = "reg:squarederror")

```



```{r xgb_grid}

# grid specification
xgboost_params <- 
  dials::parameters(
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction()
  )


```



```{r}

xgboost_grid <- 
  dials::grid_max_entropy(
    xgboost_params, 
    size = 60
  )

knitr::kable(head(xgboost_grid))

```

```{r}
lm_model <- parsnip::linear_reg()
lm_wf <- workflows::workflow() %>% 
  add_model(lm_model) %>% 
  add_formula(sim_ratings ~ .)
  
```


```{r}


xgboost_wf <- 
  workflows::workflow() %>%
  add_model(xgboost_model) %>% 
  add_formula(sim_ratings ~ .)

```


```{r message=FALSE, warning=FALSE}

metrics <- yardstick::metric_set(rmse, rsq, mae)

# hyperparameter tuning
xgboost_tuned <- tune::tune_grid(
  object = xgboost_wf,
  resamples = cv_folds,
  grid = xgboost_grid,
  metrics = metrics,
  control = tune::control_grid(verbose = TRUE)
)


```


```{r}

train_processed <- bake(rec,  new_data = training(data_split))

xgboost_train_fit <- xgboost_model %>%
  # fit the model on all the training data
  fit(
    formula = sim_ratings ~ ., 
    data    = train_processed
  ) 

xgboost_train_prediction <- xgboost_train_fit %>%
  predict(new_data = train_processed) %>%
  bind_cols(training(data_split))

lm_train_fit <- lm_model %>%
  # fit the model on all the training data
  fit(
    formula = sim_ratings ~ ., 
    data    = train_processed
  ) 

lm_train_prediction <- lm_train_fit %>%
  predict(new_data = train_processed) %>%
  bind_cols(training(data_split))

```


```{r}

xgboost_score_train <- 
  xgboost_train_prediction %>%
  yardstick::metrics(sim_ratings, .pred) %>%
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))

knitr::kable(xgboost_score_train)

```

```{r}

xgboost_test_fit <- tune::last_fit(object = xgboost_wf, 
                           split = data_split, 
                           metrics = metrics) 
knitr::kable(xgboost_test_fit$.metrics[[1]])

```

```{r}

lm_test_fit <- tune::last_fit(object = lm_wf, 
                           split = data_split, 
                           metrics = metrics) 
knitr::kable(lm_test_fit$.metrics[[1]])

```


```{r}

xgboost_train_fit %>% 
  vip::vip()

```



```{r}

lm_train_fit %>% 
  vip::vip()

```
